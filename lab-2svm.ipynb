{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Syed Sajjad Askar\n\n2139484\n\nMachine Learing\n\nLab-2\n","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries\n* pandas: Used for data manipulation and analysis\n* numpy : Numpy is the core library for scientific computing in Python. It is used for working with arrays and matrices.\n* matplotlib : It’s plotting library, and we are going to use it for data visualization\n* model_selection: Here we are going to use model_selection.train_test_split() for splitting the data\n* svm: Sklearn support vector machine model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn import svm","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:38:26.716227Z","iopub.execute_input":"2022-07-29T15:38:26.716936Z","iopub.status.idle":"2022-07-29T15:38:27.614704Z","shell.execute_reply.started":"2022-07-29T15:38:26.716880Z","shell.execute_reply":"2022-07-29T15:38:27.613723Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Load Data\n* We are going to use ‘admission_basedon_exam_scores.csv’ CSV file\n* File contains three columns Exam 1 marks, Exam 2 marks and Admission status","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('https://raw.githubusercontent.com/satishgunjal/datasets/master/admission_basedon_exam_scores.csv')\nprint('Shape of data= ', df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:38:27.616993Z","iopub.execute_input":"2022-07-29T15:38:27.617406Z","iopub.status.idle":"2022-07-29T15:38:27.850706Z","shell.execute_reply.started":"2022-07-29T15:38:27.617347Z","shell.execute_reply":"2022-07-29T15:38:27.849600Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data Understanding\n* There are total 100 training examples (m= 100 or 100 no of rows)\n* There are two features Exam 1 marks and Exam 2 marks\n* Label column contains application status. Where ‘1’ means admitted and ‘0’ means not admitted\n\n### Data Visualization\nTo plot the data of admitted and not admitted applicants, we need to first create separate data frame for each class(admitted/not-admitted)","metadata":{}},{"cell_type":"code","source":"df_admitted = df[df['Admission status'] == 1]\nprint('Training examples with admission status 1 are = ', df_admitted.shape[0])\ndf_admitted.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:38:32.401019Z","iopub.execute_input":"2022-07-29T15:38:32.401635Z","iopub.status.idle":"2022-07-29T15:38:32.420257Z","shell.execute_reply.started":"2022-07-29T15:38:32.401579Z","shell.execute_reply":"2022-07-29T15:38:32.419510Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_notadmitted = df[df['Admission status'] == 0]\nprint('Training examples with admission status 0 are = ', df_notadmitted.shape[0])\ndf_notadmitted.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:38:44.166713Z","iopub.execute_input":"2022-07-29T15:38:44.167210Z","iopub.status.idle":"2022-07-29T15:38:44.179226Z","shell.execute_reply.started":"2022-07-29T15:38:44.167176Z","shell.execute_reply":"2022-07-29T15:38:44.178470Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Now lets plot the scatter plot for admitted and not admitted students","metadata":{}},{"cell_type":"code","source":"def plot_data(title):    \n    plt.figure(figsize=(10,6))\n    plt.scatter(df_admitted['Exam 1 marks'], df_admitted['Exam 2 marks'], color= 'green', label= 'Admitted Applicants')\n    plt.scatter(df_notadmitted['Exam 1 marks'], df_notadmitted['Exam 2 marks'], color= 'red', label= 'Not Admitted Applicants')\n    plt.xlabel('Exam 1 Marks')\n    plt.ylabel('Exam 2 Marks')\n    plt.title(title)\n    plt.legend()\n \nplot_data(title = 'Admitted Vs Not Admitted Applicants')","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:38:47.139379Z","iopub.execute_input":"2022-07-29T15:38:47.139940Z","iopub.status.idle":"2022-07-29T15:38:47.369459Z","shell.execute_reply.started":"2022-07-29T15:38:47.139906Z","shell.execute_reply":"2022-07-29T15:38:47.368285Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Build Machine Learning Model","metadata":{}},{"cell_type":"code","source":"#Lets create feature matrix X and label vector y\nX = df[['Exam 1 marks', 'Exam 2 marks']]\ny = df['Admission status']\n\nprint('Shape of X= ', X.shape)\nprint('Shape of y= ', y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:38:50.536286Z","iopub.execute_input":"2022-07-29T15:38:50.536701Z","iopub.status.idle":"2022-07-29T15:38:50.547441Z","shell.execute_reply.started":"2022-07-29T15:38:50.536666Z","shell.execute_reply":"2022-07-29T15:38:50.546171Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Create Test And Train Dataset\n* We will split the dataset, so that we can use one set of data for training the model and one set of data for testing the model\n* We will keep 20% of data for testing and 80% of data for training the model\n* If you want to learn more about it, please refer [Train Test Split tutorial](https://satishgunjal.com/train_test_split/)","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state= 1)\n\nprint('X_train dimension= ', X_train.shape)\nprint('X_test dimension= ', X_test.shape)\nprint('y_train dimension= ', y_train.shape)\nprint('y_train dimension= ', y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:38:54.171491Z","iopub.execute_input":"2022-07-29T15:38:54.171879Z","iopub.status.idle":"2022-07-29T15:38:54.183668Z","shell.execute_reply.started":"2022-07-29T15:38:54.171845Z","shell.execute_reply":"2022-07-29T15:38:54.181838Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Now lets train the model using SVM classifier","metadata":{}},{"cell_type":"code","source":"# Note here we are using default SVC parameters\nclf = svm.SVC()\nclf.fit(X_train, y_train)\nprint('Model score using default parameters is = ', clf.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:38:56.181081Z","iopub.execute_input":"2022-07-29T15:38:56.181442Z","iopub.status.idle":"2022-07-29T15:38:56.194792Z","shell.execute_reply.started":"2022-07-29T15:38:56.181401Z","shell.execute_reply":"2022-07-29T15:38:56.193517Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"In order to visualize the results better lets create a function to plot SVM Classifier decision boundary with margin","metadata":{}},{"cell_type":"code","source":"def plot_support_vector(classifier):\n    \"\"\"\n    To plot decsion boundary and margin. Code taken from Sklearn documentation.\n\n    I/P\n    ----------\n    classifier : SVC object for each type of kernel\n\n    O/P\n    -------\n    Plot\n    \n    \"\"\"\n    clf =classifier\n    # plot the decision function\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # create grid to evaluate model\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = clf.decision_function(xy).reshape(XX.shape)\n\n    # plot decision boundary and margins\n    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n    # plot support vectors\n    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n               linewidth=1, facecolors='none', edgecolors='k')  ","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:38:57.876013Z","iopub.execute_input":"2022-07-29T15:38:57.878001Z","iopub.status.idle":"2022-07-29T15:38:57.887254Z","shell.execute_reply.started":"2022-07-29T15:38:57.877961Z","shell.execute_reply":"2022-07-29T15:38:57.886485Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"plot_data(title = 'SVM Classifier With Default Parameters')\nplot_support_vector(clf)  ","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:39:07.542327Z","iopub.execute_input":"2022-07-29T15:39:07.542893Z","iopub.status.idle":"2022-07-29T15:39:07.742693Z","shell.execute_reply.started":"2022-07-29T15:39:07.542860Z","shell.execute_reply":"2022-07-29T15:39:07.741571Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## SVM Parameters\n* Gamma: In case of high value of Gamma decision boundary is dependent on observations close to it, where in case of low value of Gamma, SVM will consider the far away points also while deciding the decision boundary\n* Regularization parameter(C): Large C will result in overfitting and which will lead to lower bias and high variance. Small C will result in underfitting and which will lead to higher bias and low variance. For more details about it please refer [Underfitting & Overfitting](https://satishgunjal.github.io/underfitting_overfitting/)\n\nSo regularization parameter C and gamma parameters plays an important role in order to find the best fit model. Let's create a function which will try multiple such values and return the best value of C and gamma for our choice of the kernel. At the end we will plot the decision boundary with margin using the best choice of SVM parameters for each type of kernel.","metadata":{}},{"cell_type":"code","source":"def svm_params(X_train, y_train, X_test, y_test):\n    \"\"\"\n    Finds the best choice of Regularization parameter (C) and gamma for given choice of kernel and returns the SVC object for each type of kernel\n\n    I/P\n    ----------\n    X_train : ndarray\n        Training samples\n    y_train : ndarray\n        Labels for training set\n    X_test : ndarray\n        Test data samples\n    y_test : ndarray\n        Labels for test set.\n\n    O/P\n    -------\n    classifiers : SVC object for each type of kernel\n    \n    \"\"\"\n    C_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 40]\n    gamma_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 40]\n    kernel_types = ['linear', 'poly', 'rbf']\n    classifiers = {}\n    max_score = -1\n    C_final = -1\n    gamma_final = -1\n    for kernel in kernel_types:                    \n        for C in C_values:\n            for gamma in gamma_values:\n                clf = svm.SVC(C=C, kernel= kernel, gamma=gamma)\n                clf.fit(X_train, y_train)\n                score = clf.score(X_test, y_test)\n                #print('C = %s, gamma= %s, score= %s' %(C, gamma, score))\n                if score > max_score:\n                    max_score = score\n                    C_final = C\n                    gamma_final = gamma\n                    classifiers[kernel] = clf        \n        print('kernel = %s, C = %s, gamma = %s, score = %s' %(kernel, C_final, gamma_final, max_score))\n    return classifiers","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:39:10.566413Z","iopub.execute_input":"2022-07-29T15:39:10.566791Z","iopub.status.idle":"2022-07-29T15:39:10.575796Z","shell.execute_reply.started":"2022-07-29T15:39:10.566752Z","shell.execute_reply":"2022-07-29T15:39:10.575043Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Lets call the svm_params() function to get the best parameters for each type of kernel","metadata":{}},{"cell_type":"code","source":"classifiers = svm_params(X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:39:16.702218Z","iopub.execute_input":"2022-07-29T15:39:16.702852Z","iopub.status.idle":"2022-07-29T15:39:17.885805Z","shell.execute_reply.started":"2022-07-29T15:39:16.702789Z","shell.execute_reply":"2022-07-29T15:39:17.884672Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"plot_data(title = 'SVM Classifier With Parameters ' + str(classifiers['linear']))\nplot_support_vector(classifiers['linear'])","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:39:20.166454Z","iopub.execute_input":"2022-07-29T15:39:20.166834Z","iopub.status.idle":"2022-07-29T15:39:20.361158Z","shell.execute_reply.started":"2022-07-29T15:39:20.166803Z","shell.execute_reply":"2022-07-29T15:39:20.359833Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"plot_data(title = 'SVM Classifier With Parameters ' + str(classifiers['rbf']))\nplot_support_vector(classifiers['rbf'])","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:39:22.441157Z","iopub.execute_input":"2022-07-29T15:39:22.441523Z","iopub.status.idle":"2022-07-29T15:39:22.649401Z","shell.execute_reply.started":"2022-07-29T15:39:22.441490Z","shell.execute_reply":"2022-07-29T15:39:22.648349Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"plot_data(title = 'SVM Classifier With Parameters ' + str(classifiers['poly']))\nplot_support_vector(classifiers['poly'])","metadata":{"execution":{"iopub.status.busy":"2022-07-29T15:39:23.050804Z","iopub.execute_input":"2022-07-29T15:39:23.051434Z","iopub.status.idle":"2022-07-29T15:39:23.318430Z","shell.execute_reply.started":"2022-07-29T15:39:23.051370Z","shell.execute_reply":"2022-07-29T15:39:23.317515Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nRemember that, our data is 2D so hyperplane will be a line. But if you observe the data closely there is no clear separation between classes that's why straight line is not a good fit, which is obvious from above plots. Though the accuracy of poly kernel is less than rbf, but still its best choice for our data. \n\n","metadata":{}}]}